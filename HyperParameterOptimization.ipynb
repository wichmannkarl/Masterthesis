{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538c9c7a",
   "metadata": {},
   "source": [
    "# Masterthesis\n",
    "## Hyperparameter Optimization\n",
    "\n",
    "This script find out the hyperparameter for the each machine learning algorithmus and scenario.  \n",
    "HalvingGridSearch was used to determine the hyperparameters. It is also possible to determine the hyperparameters for individual machine learning algorithms and a single scenario.\n",
    "\n",
    "**Imports and Definitions**\n",
    "- The necessary libraries are loaded here and important variables are defined\n",
    "\n",
    "**Imports and settings for this script**\n",
    "- Import libraries and set variables for this script\n",
    "\n",
    "**Preparation for Hyperparameter Search with HalvingGridSearchCV**\n",
    "- Define the machine learning algorithm, set parameter and create pipline for the HalvingGridSearch\n",
    "\n",
    "**HalvingGridSearch**\n",
    "- Read Scenario config files and run all Scenarios with the estimatiors to derterminate Hyperparameter\n",
    "\n",
    "**HalvingGridSearch for a single file**\n",
    "- Used to Search Hyperparameter for a single Scenario file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48449f57",
   "metadata": {},
   "source": [
    "## Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091754c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn\n",
    "import sklearn\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# To calculate amplitude and phase\n",
    "import math\n",
    "\n",
    "# Measure runtime of a jupyter jotebook code cell\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Used to check if file exists\n",
    "import os\n",
    "\n",
    "# Used to check if directory exists\n",
    "import pathlib\n",
    "\n",
    "# Import Operation System Calls\n",
    "import SubOperationSystem\n",
    "\n",
    "# check os\n",
    "if os.name == 'nt':\n",
    "    print(\"OS is Windows\")\n",
    "    Delimiter = '\\\\'\n",
    "    \n",
    "else:\n",
    "    print(\"OS is Linux\")\n",
    "    Delimiter = '/'\n",
    "    \n",
    "# Path of datasets (root directory)\n",
    "PathDataset = 'Dataset' + Delimiter    \n",
    "\n",
    "# Path of datasets\n",
    "PathDatasetSub = PathDataset + 'CsiFilesRah' + Delimiter\n",
    "        \n",
    "# Path of the converted files\n",
    "PathConverted = PathDataset + 'Converted' + Delimiter\n",
    "\n",
    "# Set path for scenario files\n",
    "PathScenario = PathDataset + 'Scenario' + Delimiter\n",
    "\n",
    "# Set path for scenario files\n",
    "PathResult = PathDataset + 'Result' + Delimiter\n",
    "\n",
    "# Set path for scenario files\n",
    "PathPlot = PathDataset + 'Plot' + Delimiter\n",
    "\n",
    "# Set path for scenario files\n",
    "PathConfig = 'FilesConfig' + Delimiter\n",
    "\n",
    "# Scenariofile (file with info about the ten scenarios)\n",
    "FileScenario = 'FileScenario.csv'\n",
    "\n",
    "# Mappingfile (file with info about original and converted filenames)\n",
    "FileMapping = 'FileMapping.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca2a490",
   "metadata": {},
   "source": [
    "# Imports and settings for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Classifier RF and GBC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Import GradientBoostingClassifier \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Import Support Vektor Machine\n",
    "from sklearn import svm\n",
    "\n",
    "# Import HalvingGridSearch\n",
    "from sklearn.experimental import enable_halving_search_cv \n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# Import make_classification\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Import Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Import AccuraceScore\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Import PCA for Dimenstion Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Import decomposition\n",
    "from sklearn import decomposition\n",
    "\n",
    "# Import standardscaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import f1_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Import explainable boosting machine\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "# Import XGBoosting classifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3689f50",
   "metadata": {},
   "source": [
    "# Preparation for Hyperparameter Search with HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd95391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counter for estiminator name\n",
    "EstimatorNamesCounter = 0\n",
    "\n",
    "# Set level for verbose\n",
    "vb = 0\n",
    "\n",
    "# Set Count of crossvalidation\n",
    "cv = 3\n",
    "\n",
    "# Set random state\n",
    "rs = 10\n",
    "\n",
    "# Set Type of scoring\n",
    "scoring='accuracy'\n",
    "\n",
    "# Range of parameter\n",
    "ParameterRange = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# Range of Parameter only for Logistic Regression\n",
    "ParameterRangeLR = [1.0, 0.5, 0.1]\n",
    "\n",
    "# Set Count for estimators\n",
    "ParametersEstimators = [50,100,150,200,250]\n",
    "\n",
    "# Set Counts of interations\n",
    "ParameterInterations = 2500\n",
    "\n",
    "# PCA parameter\n",
    "ParameterPca = 30\n",
    "\n",
    "# Set Used cores for estimators\n",
    "jobs = 24\n",
    "\n",
    "# EBM\n",
    "ParameterEbmMaxBins =  [256,512,1024]\n",
    "ParameterEbmMaxInteractionBins =  [16, 32, 64]\n",
    "ParameterEbmMaxRounds =  [5000]\n",
    "ParameterEbmLearningRate = [0.02, 0.01, 0.005]\n",
    "ParameterEbmMinSampleLeaf = [2, 3]\n",
    "ParameterEbmOuterBags = [14, 25, 50, 75, 100]\n",
    "ParameterEbmValidationSize = [0.1, 0.15, 0.2]\n",
    "\n",
    "# XGB\n",
    "ParameterXgbeta = [0.01, 0.1, 0.3, 0.5]\n",
    "ParameterXgbmax_depth = [1, 3, 5, 7, 9]\n",
    "ParameterXgbmin_child_weight = [1, 3, 5, 7]\n",
    "ParameterXgbsubsample = [0.2, 0.5, 0.7, 1.0]\n",
    "ParameterXgbcolsample_bytree = [0.5, 0.7, 1.0]\n",
    "ParameterXgbgamma = [0, 0.1, 0.3, 0.5]\n",
    "ParameterXgbn_estimators = [10, 50, 100, 200]\n",
    "\n",
    "# Define estimator names\n",
    "EstimatorNames = {\n",
    "    0: 'RandomForestClassifier', \n",
    "    1: 'LogisticRegression', \n",
    "    2: 'KNeighborsClassifier', \n",
    "    3: 'SVC', \n",
    "    4: 'GradientBoostingClassifier',\n",
    "    5: 'ExplainableBoostingClassifier',\n",
    "    6: 'XGBClassifier'\n",
    "}\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# set grip parameter for RandomForestClassifier\n",
    "ParamRFC = [\n",
    "    {\n",
    "        'RF__min_samples_leaf': ParameterRange,\n",
    "        'RF__min_samples_split': ParameterRange[1:],\n",
    "        'RF__max_depth': ParameterRange,\n",
    "        'RF__n_estimators': ParametersEstimators\n",
    "    }\n",
    "]\n",
    "\n",
    "# set grip parameter for Logistic Regression\n",
    "ParamLR = [\n",
    "    {\n",
    "        'LR__C': ParameterRangeLR,\n",
    "        'LR__solver': ['liblinear'],\n",
    "        'LR__penalty': ['l1', 'l2'],\n",
    "        'LR__max_iter': [ParameterInterations]\n",
    "    }\n",
    "]\n",
    "\n",
    "# set grip parameter for K-NeighborsClassifier\n",
    "ParamKNN = [\n",
    "    {\n",
    "        'KNN__weights': ['uniform', 'distance'],\n",
    "        'KNN__metric': ['euclidean', 'manhattan'],\n",
    "        'KNN__n_neighbors': ParameterRange,\n",
    "    }\n",
    "]\n",
    "\n",
    "# set grip parameter for Support Vektor Machine\n",
    "ParamSVC = [\n",
    "    {\n",
    "        'SVM__C': ParameterRange,\n",
    "        'SVM__gamma': ['auto'],\n",
    "        'SVM__kernel': ['linear', 'rbf'], \n",
    "    }\n",
    "]\n",
    "\n",
    "# set grip parameter for GradientBoostingClassifier\n",
    "ParamGBC = [\n",
    "    {\n",
    "        'GBC__min_samples_leaf': ParameterRange,\n",
    "        'GBC__max_depth': ParameterRange,\n",
    "        'GBC__min_samples_split': ParameterRange[1:]\n",
    "    }\n",
    "]\n",
    "\n",
    "# set grip parameter for ExplainableBoostingMachine\n",
    "ParamEBM = [\n",
    "    {\n",
    "        'EBM__max_bins': ParameterEbmMaxBins,\n",
    "        'EBM__max_interaction_bins': ParameterEbmMaxInteractionBins,\n",
    "        'EBM__max_rounds': ParameterEbmMaxRounds,\n",
    "        'EBM__learning_rate': ParameterEbmLearningRate,\n",
    "        'EBM__min_samples_leaf': ParameterEbmMinSampleLeaf,\n",
    "        'EBM__outer_bags': ParameterEbmOuterBags,\n",
    "        'EBM__validation_size': ParameterEbmValidationSize\n",
    "    }    \n",
    "]\n",
    "\n",
    " # set grip parameter for XGBoosting\n",
    "ParamXGB = [\n",
    "    {   \n",
    "        'XGB__eta': ParameterXgbeta,\n",
    "        'XGB__max_depth': ParameterXgbmax_depth,\n",
    "        'XGB__min_child_weight': ParameterXgbmin_child_weight,\n",
    "        'XGB__subsample': ParameterXgbsubsample,\n",
    "        'XGB__colsample_bytree': ParameterXgbcolsample_bytree,\n",
    "        'XGB__gamma': ParameterXgbgamma,\n",
    "        'XGB__n_estimators': ParameterXgbn_estimators\n",
    "    }\n",
    "]\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Pipelines for RandomForestClassifier\n",
    "PipelineRF = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('RF',RandomForestClassifier(random_state=rs))]\n",
    ")\n",
    "\n",
    "# Pipelines for LogisticRegression\n",
    "PipelineLR = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('LR', LogisticRegression(random_state=rs))]\n",
    ")\n",
    "\n",
    "# Pipelines for KNeighborsClassifier\n",
    "PipelineKNN = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('KNN', KNeighborsClassifier())]\n",
    ")\n",
    "\n",
    "# Pipelines for SVM\n",
    "PipelineSVM = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('SVM', svm.SVC(random_state=rs))]\n",
    ")\n",
    "\n",
    "# Pipelines for GradientBoostingClassifier\n",
    "PipelineGBC = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('GBC', GradientBoostingClassifier(random_state=rs))]\n",
    ")\n",
    "    \n",
    "# Pipelines for ExplainableBoostingClassifier\n",
    "PipelineEBM = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('EBM', ExplainableBoostingClassifier(random_state=rs, interactions=0))]\n",
    ")\n",
    "\n",
    "# Pipelines for RandomForestClassifier\n",
    "PipelineXGB = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('XGB',xgb.XGBClassifier(objective='multi:softprob', use_label_encoder=False, eval_metric='mlogloss', num_class=3))]\n",
    ")\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "# Search for Random Forest Classifier\n",
    "SearchRF = HalvingGridSearchCV(estimator=PipelineRF, param_grid=ParamRFC, scoring=scoring, cv=cv, verbose=vb, n_jobs=jobs)\n",
    "\n",
    "# Seach for Logistic Regression\n",
    "SearchLR = HalvingGridSearchCV(estimator=PipelineLR, param_grid=ParamLR, scoring=scoring,cv=cv, verbose=vb)\n",
    "\n",
    "# Search for K-Neighbors Classifier \n",
    "SearchKNN = HalvingGridSearchCV(estimator=PipelineKNN, param_grid=ParamKNN, scoring=scoring, cv=cv, verbose=vb)\n",
    "\n",
    "# Search for Support Vektor Machine\n",
    "SearchSVM = HalvingGridSearchCV(estimator=PipelineSVM, param_grid=ParamSVC, scoring=scoring, cv=cv, verbose=vb, n_jobs=jobs)\n",
    "\n",
    "# Search for Gradient Boosting Classifier\n",
    "SearchGBC = HalvingGridSearchCV(estimator=PipelineGBC, param_grid=ParamGBC, scoring=scoring, cv=cv, verbose=vb, n_jobs=jobs)\n",
    "\n",
    "# Search for Explainable Boosting Machine\n",
    "# SearchEBM = ExplainableBoostingClassifier(estimator=PipelineEBM, param_grid=ParamEBM, scoring=scoring, cv=cv, verbose=vb, n_jobs=jobs)\n",
    "SearchEBM = HalvingGridSearchCV(estimator=PipelineEBM, param_grid=ParamEBM, scoring=scoring, cv=cv, verbose=vb, n_jobs=jobs)\n",
    "\n",
    "# Search for XGBoosting\n",
    "SearchXGB = HalvingGridSearchCV(estimator=PipelineXGB, param_grid=ParamXGB, scoring=scoring, cv=cv, verbose=vb)\n",
    "    \n",
    "#####################################################################################################\n",
    "\n",
    "# List with estimatiors\n",
    "SearchGrid = [SearchRF, SearchLR, SearchKNN, SearchSVM, SearchGBC, SearchEBM, SearchXGB]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e01b11",
   "metadata": {},
   "source": [
    "# HalvingGridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open scenario mapping file to read\n",
    "dfScenario = pd.read_csv(PathConfig + FileScenario, index_col=0)\n",
    "\n",
    "# loop through scenario dataframe\n",
    "for ind in dfScenario.index:\n",
    "\n",
    "    # get scenarios and dataset from dataframe\n",
    "    Scenario = (dfScenario['Scenario'][ind])\n",
    "    \n",
    "    # load scenario file\n",
    "    df = pd.read_csv(PathScenario + Scenario + '_ah.csv')\n",
    "\n",
    "    # Select data and remove target\n",
    "    X = df.drop(\"label\",axis=1)\n",
    "\n",
    "    # Select target\n",
    "    y = df['label']\n",
    "\n",
    "    # Split dataset to train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "    # Print line\n",
    "    print(80 * '=')\n",
    "\n",
    "    # Print scenario name\n",
    "    print (Scenario)\n",
    "\n",
    "    # Print line\n",
    "    print(80 * '-')\n",
    "\n",
    "    # Set counter to 0\n",
    "    counter = 0\n",
    "\n",
    "    # loop through SearchGrid\n",
    "    for pipe in SearchGrid:\n",
    "\n",
    "        # Fit model\n",
    "        pipe.fit(X_train,y_train)\n",
    "\n",
    "        # Print results\n",
    "        print('{} Test Accuracy: {}'.format(EstimatorNames[counter], pipe.score(X_test,y_test)))\n",
    "        print('{} Best Params: {}'.format(EstimatorNames[counter], pipe.best_params_))\n",
    "        print(80 * '-')    \n",
    "        \n",
    "        # Increase counter for estimatior name\n",
    "        counter +=1\n",
    "    \n",
    "    # Print line\n",
    "    print(60 * '-')\n",
    "\n",
    "    # Increase estimatior counter\n",
    "    EstimatorNamesCounter +=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675bad1b",
   "metadata": {},
   "source": [
    "# HalvingGridSearch for a single file and estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93c2107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Settings for this single script\n",
    "# Beginn\n",
    "\n",
    "# First: set scenario file\n",
    "Scenario = 'Scenario01'\n",
    "\n",
    "# Second: define estimator\n",
    "SearchGrid = [SearchXGB]\n",
    "\n",
    "# Third: set True if estimator is XGB\n",
    "CheckEstimatorXgb = True\n",
    "\n",
    "# Counter for Estimator name\n",
    "# 0: 'RandomForestClassifier', \n",
    "# 1: 'LogisticRegression', \n",
    "# 2: 'KNeighborsClassifier', \n",
    "# 3: 'SVC', \n",
    "# 4: 'GradientBoostingClassifier',\n",
    "# 5: 'ExplainableBoostingClassifier',\n",
    "# 6: 'XGBClassifier\n",
    "counter = 6\n",
    "\n",
    "# End\n",
    "# ----------------------------------------------------------------------------------------\n",
    "\n",
    "# load scenario file\n",
    "df = pd.read_csv(PathScenario + Scenario + '_ah.csv')\n",
    "\n",
    "# Select data and remove target\n",
    "X = df.drop(\"label\", axis = 1)\n",
    "\n",
    "# XGB need other labels\n",
    "# xgboost needs labels from 0 to ... n\n",
    "if CheckEstimatorXgb:\n",
    "\n",
    "    # First: get names of unique classes and save it into classes\n",
    "    classes = df['label'].unique()\n",
    "\n",
    "    # Second: create dictionary with original y values\n",
    "    classesDictionary = {i: value for i, value in enumerate(classes)}\n",
    "\n",
    "    # Third: replace y-value with values beginning from 0 to ...\n",
    "    for key, value in classesDictionary.items():\n",
    "        df.loc[df['label'] == value, 'label'] = key\n",
    "\n",
    "# Select target\n",
    "y = df['label']\n",
    "\n",
    "# Split dataset to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)\n",
    "\n",
    "# Print line\n",
    "print(80 * '=')\n",
    "\n",
    "# Print Scenario name\n",
    "print (Scenario)\n",
    "\n",
    "# Print line\n",
    "print(80 * '-')\n",
    "\n",
    "# Fit model\n",
    "pipe = SearchGrid[0]\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "# Print results\n",
    "print('{} Test Accuracy: {}'.format(EstimatorNames[counter], pipe.score(X_test,y_test)))\n",
    "print('{} Best Params: {}'.format(EstimatorNames[counter], pipe.best_params_))\n",
    "print(80 * '-')    \n",
    "\n",
    "# Print line\n",
    "print(60 * '-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c44a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
