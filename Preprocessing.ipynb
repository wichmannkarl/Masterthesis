{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538c9c7a",
   "metadata": {},
   "source": [
    "# Masterthesis\n",
    "## Preprocessing\n",
    "\n",
    "**Imports and Definitions**\n",
    "- The necessary libraries are loaded here and important variables are defined\n",
    "\n",
    "**Imports and settings for this script**\n",
    "- Import libraries and set variables for this script\n",
    "\n",
    "**Check directory and nessesary config files**\n",
    "- Create output directorys for results if needed\n",
    "\n",
    "**Read original CSI-Files, clean up data and saved with a new filename**\n",
    "- In this step, the original CSI file is read, cleaned up and written to the \"PathConverted\" directory with a new file name. The configuration file for the mapping is called \"FileMapping\" and contains the line number, FilenameOld and FilenameNew.\n",
    "\n",
    "**Calculate amplitude**\n",
    "- The amplitude value of the CSI data is required for the next steps. This is calculated from the imaginary and real values. The already cleaned CSI data is loaded for this purpose, the amplitude is calculated and saved as a new file with the extension \"_ah.csv\" in the \"PathConverted\" directory\n",
    "\n",
    "**Calculate windowssize of hampel filter**\n",
    "- The hampel filter is used to remove outliers in the CSI data. To do this, it is necessary to calculate the value of windowssize in advance. This step has been outsourced to the \"PreprocessingControll.ipynb\" file, as this value only needs to be calculated once. The calculated value of \"6\" is used in the following step to remove the outliers.\n",
    "\n",
    "**Create Scenario for Machine Learning**\n",
    "- The last step is to create the scenario file. These ultimately contain the scenarios, which are then analyzed using machine learning algorithms. The files are saved in the \"PathScenario\" directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48449f57",
   "metadata": {},
   "source": [
    "## Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "091754c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OS is Windows\n"
     ]
    }
   ],
   "source": [
    "# Import sklearn\n",
    "import sklearn\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# To calculate amplitude and phase\n",
    "import math\n",
    "\n",
    "# Measure runtime of a jupyter jotebook code cell\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Used to check if file exists\n",
    "import os\n",
    "\n",
    "# Used to check if directory exists\n",
    "import pathlib\n",
    "\n",
    "# Import Operation System Calls\n",
    "import SubOperationSystem\n",
    "\n",
    "# check os\n",
    "if os.name == 'nt':\n",
    "    print(\"OS is Windows\")\n",
    "    Delimiter = '\\\\'\n",
    "    \n",
    "else:\n",
    "    print(\"OS is Linux\")\n",
    "    Delimiter = '/'\n",
    "    \n",
    "# Path of datasets (root directory)\n",
    "PathDataset = 'Dataset' + Delimiter    \n",
    "\n",
    "# Path of datasets\n",
    "PathDatasetSub = PathDataset + 'CsiFilesRah' + Delimiter\n",
    "        \n",
    "# Path of the converted files\n",
    "PathConverted = PathDataset + 'Converted' + Delimiter\n",
    "\n",
    "# Set path for scenario files\n",
    "PathScenario = PathDataset + 'Scenario' + Delimiter\n",
    "\n",
    "# Set path for scenario files\n",
    "PathResult = PathDataset + 'Result' + Delimiter\n",
    "\n",
    "# Set path for scenario files\n",
    "PathPlot = PathDataset + 'Plot' + Delimiter\n",
    "\n",
    "# Set path for scenario files\n",
    "PathConfig = 'FilesConfig' + Delimiter\n",
    "\n",
    "# Scenariofile (file with info about the ten scenarios)\n",
    "FileScenario = 'FileScenario.csv'\n",
    "\n",
    "# Mappingfile (file with info about original and converted filenames)\n",
    "FileMapping = 'FileMapping.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709ac13",
   "metadata": {},
   "source": [
    "# Imports and settings for this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3317f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ScenariofilePre - needed to create Scenario10\n",
    "FileScenarioPre = 'FileScenarioPreconvert.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99904414",
   "metadata": {},
   "source": [
    "## Check directory and nessesary config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e828fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directroy \"PathConverted\" if not exists\n",
    "pathlib.Path(PathConverted).mkdir(parents = True, exist_ok = True)\n",
    "  \n",
    "# Create directroy \"PathResult\" if not exists\n",
    "pathlib.Path(PathResult).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# Create directroy \"PathScenario\" if not exists\n",
    "pathlib.Path(PathScenario).mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "# check if mapping file exists else exists\n",
    "if not SubOperationSystem.checkIfFileExists(FileMapping, True):\n",
    "    exit()\n",
    "    \n",
    "# check if scenario file exists else exists\n",
    "if not SubOperationSystem.checkIfFileExists(FileScenario, True):\n",
    "    exit()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667fc15",
   "metadata": {},
   "source": [
    "## Scenario10\n",
    "\n",
    "The scenario10 file is combination of diffent CSI-Rahfile. This Files are needed for the Scenario10. Details see in PreprocessingCreateSpezialFiles.ipynb\n",
    "\n",
    "Call here **PreprocessingCreateSpezialFiles.ipynb** if nessesary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b5bd9",
   "metadata": {},
   "source": [
    "## Read original CSI-Files, clean up data and saved with a new filename\n",
    "\n",
    "- Allow only permitted MAC addresses\n",
    "- Remove spaces and special character\n",
    "- Select only nessary colums \n",
    "- Calculate amplitude\n",
    "- Save result with the file extension \"_a.csv\" the \"PathResult\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b302e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config file \"FileMapping\"\n",
    "dfFiles = pd.read_csv(PathConfig + FileMapping)\n",
    "\n",
    "# loop through config file \"FileMapping\"\n",
    "for ind in dfFiles.index:\n",
    "        \n",
    "    # read Linenumber, FilenameOld and Filenamenew\n",
    "    LineNumber,FilenameOld,FilenameNew = (dfFiles['LineNumber'][ind], dfFiles['FilenameOld'][ind], dfFiles['FilenameNew'][ind])\n",
    "\n",
    "    # Set Delimiter for Linux (only for FilenameOld)\n",
    "    if Delimiter == \"/\":\n",
    "        FilenameOld = FilenameOld.replace(\"\\\\\\\\\", \"/\")\n",
    "    \n",
    "    # check if \"FilenameOld\" file exists\n",
    "    if not SubOperationSystem.checkIfFileExists(PathDatasetSub + FilenameOld, True):\n",
    "        exit()\n",
    "        \n",
    "    else:\n",
    "        # if output file exists -> next file\n",
    "        if SubOperationSystem.checkIfFileExists(PathConverted + FilenameNew, False):\n",
    "            continue\n",
    "    \n",
    "    # read dataset\n",
    "    df = pd.read_csv(PathDatasetSub + FilenameOld)\n",
    "    \n",
    "    # query only allowed mac addresses\n",
    "    df.query('source_mac_addr in [\"ec:94:cb:6e:73:8c\", \"ec:94:cb:6e:7c:64\"]',inplace=True)\n",
    "    \n",
    "    # use only columns with important CSI-Data\n",
    "    df = pd.concat([df.loc[:,'csi_subcarrier_6':'csi_subcarrier_31'],df.loc[:,'csi_subcarrier_33':'csi_subcarrier_58']],axis=1)\n",
    "\n",
    "    # replace \"(\"\n",
    "    df = df.replace('\\(','', regex=True)\n",
    "\n",
    "    # replace \")\"\n",
    "    df = df.replace('\\)','', regex=True)\n",
    "    \n",
    "    # remove spaces\n",
    "    df = df.replace('\\ ','', regex=True)\n",
    "    \n",
    "    # set value of label\n",
    "    df.loc[:, [\"label\"]] = LineNumber\n",
    "\n",
    "    # export new file with \"FilenameNew\" without index\n",
    "    df.to_csv(PathConverted + FilenameNew, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e75c8",
   "metadata": {},
   "source": [
    "## Calculate amplitude\n",
    "\n",
    "- In this section we calculate the amplitude of the csi file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d3de13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through config file \"FileMapping\"\n",
    "for ind in dfFiles.index:\n",
    "        \n",
    "    # get filenames\n",
    "    FilenameNew = (dfFiles['FilenameNew'][ind])\n",
    "    \n",
    "    # rename filename with amplitude label\n",
    "    FILEEXTENSION = FilenameNew.replace(\".csv\", \"_a.csv\")\n",
    "\n",
    "    # first check if file exists\n",
    "    if not SubOperationSystem.checkIfFileExists(PathConverted + FilenameNew, True):\n",
    "        exit()\n",
    "    \n",
    "    else:\n",
    "        # if output file exists -> next file\n",
    "        if SubOperationSystem.checkIfFileExists(PathConverted + FILEEXTENSION, False):\n",
    "            continue\n",
    "    \n",
    "    # read converted dataset\n",
    "    dfAmplitude = pd.read_csv(PathConverted + FilenameNew)\n",
    "    \n",
    "    # get count of columns (the last column is the label)\n",
    "    ENDCOLUMN = len(dfAmplitude.columns)-1\n",
    "\n",
    "    # loop through rows\n",
    "    for MAINCOUNTER in range(len(dfAmplitude)):\n",
    "        \n",
    "        # loop through columns\n",
    "        for COUNTER in range(0, ENDCOLUMN):\n",
    "        \n",
    "            # split values in cell, separated by comma\n",
    "            (IMAGINAR,REAL) = (dfAmplitude.iloc[MAINCOUNTER, COUNTER]).split(',')\n",
    "        \n",
    "            # calculate amplitude and phase\n",
    "            AMPLITUDE = (round(math.sqrt(float(IMAGINAR)** 2 + float(REAL)** 2),4))\n",
    "            PHASE = (round(math.atan2(float(IMAGINAR), float(REAL)),4))\n",
    "            \n",
    "            # set new value\n",
    "            dfAmplitude.iloc[MAINCOUNTER, COUNTER] = AMPLITUDE\n",
    "            \n",
    "    # Export dataset without index and other file name\n",
    "    dfAmplitude.to_csv(PathConverted + FILEEXTENSION, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fa6cd",
   "metadata": {},
   "source": [
    "## Value for Hampelfilter\n",
    "\n",
    "The script \"PreprocessingControll.ipynb\" was executed to determine the value for the hampel filter. The calculated value is 6 and was used in the next step. See \"PreprocessingControll.ipynb\" for more details.\n",
    "\n",
    "\n",
    "Call **PreprocessingControll.ipynb** if nessessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2186291b",
   "metadata": {},
   "source": [
    "# Remove outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13037512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hampel filter\n",
    "from hampel import hampel\n",
    "\n",
    "# loop through config file \"FileMapping\"\n",
    "for ind in dfFiles.index:\n",
    "    \n",
    "    # get filenames\n",
    "    LineNumber,FilenameNew = (dfFiles['LineNumber'][ind], dfFiles['FilenameNew'][ind])\n",
    "        \n",
    "    # rename filename with ampitude label\n",
    "    DataFileAmplitude = FilenameNew.replace(\".csv\", \"_a.csv\")\n",
    "    \n",
    "    # check if file exists\n",
    "    if not SubOperationSystem.checkIfFileExists(PathConverted + DataFileAmplitude, True):\n",
    "        exit()\n",
    "\n",
    "    else:\n",
    "        # rename filename with hampel label\n",
    "        DataFileFilter = FilenameNew.replace(\".csv\", \"_ah.csv\")\n",
    "\n",
    "        # if output file exists -> next file\n",
    "        if SubOperationSystem.checkIfFileExists(PathConverted + Delimiter + DataFileFilter, False):\n",
    "            continue\n",
    "\n",
    "    # read csv file to dataframe\n",
    "    dfCSI = pd.read_csv(PathConverted + Delimiter + DataFileAmplitude)\n",
    "\n",
    "    # copy df to numpy array, because hampel need numpy array\n",
    "    ArrayCsiUnfiltered = dfCSI.to_numpy()\n",
    "    ArrayCsiFiltered = dfCSI.to_numpy()\n",
    "\n",
    "    # hampel filter to remove outlier\n",
    "    for y in range(dfCSI.shape[1]):\n",
    "        ArrayCsiFiltered[:,y] = hampel(ArrayCsiUnfiltered[:,y],window_size=6, n_sigma=3.0).filtered_data\n",
    "\n",
    "        # Round hampel values from 15 to 4 decimal places\n",
    "        ArrayCsiFilteredRounded = np.round(ArrayCsiFiltered, 4)\n",
    "    \n",
    "    # add columnnames\n",
    "    dfFilter = pd.DataFrame(ArrayCsiFilteredRounded,columns=(list(dfCSI.columns.values.tolist())))\n",
    "    \n",
    "    # add label column and add value of label\n",
    "    dfFilter['label'] = None\n",
    "    dfFilter['label'] = dfFilter['label'].fillna(LineNumber)\n",
    "    \n",
    "    # save pandas dataframe to file\n",
    "    dfFilter.to_csv(PathConverted + DataFileFilter, index=False)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6409df",
   "metadata": {},
   "source": [
    "## Create Scenario for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3264deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open scenario mapping file to read\n",
    "dfScenario = pd.read_csv(PathConfig + FileScenario, index_col=0)\n",
    "\n",
    "# Read Config File: files\n",
    "dfFiles = pd.read_csv(PathConfig + FileMapping, index_col=0)\n",
    "\n",
    "# loop through scenario dataframe\n",
    "for ind in dfScenario.index:\n",
    "    \n",
    "    # get scenarios and dataset from dataframe\n",
    "    Scenario,Datasets = (dfScenario['Scenario'][ind], dfScenario['Datasets'][ind])\n",
    "\n",
    "    # create scenario file with all scenario datasets to write\n",
    "    FileScenario = open(PathScenario + Scenario + \"_ah.csv\", 'w')\n",
    "    \n",
    "    # split to dataset items because we need a int value\n",
    "    DatasetItems=list(Datasets.split())\n",
    "    \n",
    "    # loop through list\n",
    "    for DatasetItem in DatasetItems:\n",
    "    \n",
    "        # filenames of needed dataset in the column 'FilenameNew'\n",
    "        DatasetFilenames = dfFiles.loc[int(DatasetItem)]['FilenameNew']\n",
    "                \n",
    "        # rename filename with amplitude label\n",
    "        FileDataHampel = DatasetFilenames.replace(\".csv\", \"_ah.csv\")\n",
    "                \n",
    "        # open Dataset to read\n",
    "        FileFilter = open(PathConverted + FileDataHampel, \"r\")\n",
    "        \n",
    "        # create dataframe hampel\n",
    "        dfFilter = pd.read_csv(FileFilter)\n",
    "        \n",
    "        # add label column and add value of label\n",
    "        dfFilter['label'] = None\n",
    "        dfFilter['label'] = dfFilter['label'].fillna(DatasetItem)\n",
    "        \n",
    "        # append DataFrame to file\n",
    "        if FileScenario.tell() == 0:\n",
    "            # if file is empty write header\n",
    "            dfFilter.to_csv(FileScenario, index=False, line_terminator='\\n')\n",
    "        else:\n",
    "            dfFilter.to_csv(FileScenario, index=False, line_terminator='\\n', header=False)\n",
    "                    \n",
    "        # close file to read\n",
    "        FileFilter.close()\n",
    "        \n",
    "    # close scenario file\n",
    "    FileScenario.close()  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
